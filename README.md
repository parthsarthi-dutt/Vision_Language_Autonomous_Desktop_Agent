```txt
#ü§ñ Vision AI Assistant - Intelligent Screen Automation

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyQt5](https://img.shields.io/badge/PyQt5-5.15+-green.svg)
![Gemini](https://img.shields.io/badge/Gemini-2.5--Flash-orange.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

**An AI-powered virtual assistant that understands your screen and automates complex tasks through voice commands and visual understanding.**

[Features](#-features) ‚Ä¢ [Demo](#-demo) ‚Ä¢ [Installation](#-installation) ‚Ä¢ [Usage](#-usage) ‚Ä¢ [Architecture](#-architecture) ‚Ä¢ [Configuration](#-configuration)

</div>

```

##üìã Table of Contents

- [Overview](#-overview)
- [Key Features](#-features)
- [System Architecture](#-architecture)
- [How It Works](#-how-it-works)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Usage Guide](#-usage-guide)
- [API Integration](#-api-integration)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)

```
##üåü Overview

Vision AI Assistant is a breakthrough automation tool that combines **computer vision**, **natural language processing**, and **multimodal AI** to understand and interact with your computer screen like a human would. Unlike traditional automation tools that rely on brittle selectors or coordinates, this assistant **sees** your screen, **understands** the context, and **executes** tasks intelligently.

###üéØ What Makes It Special?

- **üîç Visual Understanding**: Uses OmniParser to detect and parse ALL UI elements on screen
- **üß† AI Decision Making**: Powered by Google's Gemini 2.5 Flash for intelligent action planning
- **üé§ Voice Control**: Natural language voice commands with wake word detection
- **üîÑ Adaptive Execution**: Monitors screen changes and adapts strategy in real-time
- **üé® Beautiful UI**: Modern, translucent overlay interface with status indicators

```
##‚ú® Features

###üñºÔ∏è Screen Understanding
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Your Screen                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ [1]  ‚îÇ  ‚îÇ   [2]    ‚îÇ  ‚îÇ   [3]   ‚îÇ      ‚îÇ
‚îÇ  ‚îÇChrome‚îÇ  ‚îÇ  Search  ‚îÇ  ‚îÇ  Login  ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ         ‚Üì OmniParser Analysis ‚Üì            ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  Element [1]: Chrome Browser Icon           ‚îÇ
‚îÇ  Element [2]: Search Input Field            ‚îÇ
‚îÇ  Element [3]: Login Button                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

###üé§ Voice Input System

**Two Activation Modes:**

1. **Wake Word Detection**
   ```
   User: "Hey Vision"
   System: üé§ [Listening...]
   User: "Play Lata Mangeshkar songs"
   System: ‚úÖ [Executing task...]
   ```

2. **Manual Button Activation**
   ```
   User: [Clicks üéôÔ∏è Speak button]
   System: üé§ [Listening...]
   User: "Open Chrome and search for AI news"
   System: ‚úÖ [Executing task...]
   ```

**Voice Recognition Modes:**
- **Online Mode** (Default): Google Speech Recognition - High accuracy, requires internet
- **Offline Mode**: Sphinx Recognition - Works offline, lower accuracy

###ü§ñ AI-Powered Task Execution

The assistant breaks down complex tasks into atomic steps:

```
Task: "Play Lata Mangeshkar songs on YouTube"
         ‚Üì
    [AI Analysis]
         ‚Üì
Step 1: Click Chrome icon [Element 5]
         ‚Üì
Step 2: Wait for browser to open
         ‚Üì
Step 3: Click address bar [Element 12]
         ‚Üì
Step 4: Type "youtube.com"
         ‚Üì
Step 5: Wait for page load
         ‚Üì
Step 6: Click search box [Element 8]
         ‚Üì
Step 7: Type "Lata Mangeshkar songs"
         ‚Üì
Step 8: Wait for results
         ‚Üì
Step 9: Click first video [Element 3]
         ‚Üì
    [Task Complete ‚úÖ]
```

###üìä Real-Time Monitoring

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Screen Change Detection System             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                             ‚îÇ
‚îÇ  Action Performed ‚Üí Monitor Screen          ‚îÇ
‚îÇ         ‚Üì                                   ‚îÇ
‚îÇ  Calculate Pixel Difference                 ‚îÇ
‚îÇ         ‚Üì                                   ‚îÇ
‚îÇ  Threshold Check (5% default)               ‚îÇ
‚îÇ         ‚Üì                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇChanged? ‚îÇ   YES   ‚îÇ Wait    ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ Buffer  ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ       ‚îÇ NO                 ‚îÇ               ‚îÇ
‚îÇ       ‚Üì                    ‚Üì               ‚îÇ
‚îÇ  Keep Checking      Proceed to Next Step   ‚îÇ
‚îÇ  (Max 4s timeout)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

###üé® User Interface

**Main Input Window**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Ready - Enter a task (or say 'Hey Vision')    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                 ‚îÇ
‚îÇ  Type or say: "Hey Vision, play music"  [üéôÔ∏è]  ‚îÇ
‚îÇ                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Status Overlay** (Bottom Center)
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üîÑ Analyzing Screen...         ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ       ‚îÇ
‚îÇ  Found 47 elements ‚Ä¢ Asking AI  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Control Panel** (Right Side)
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Abort   ‚îÇ ‚Üê Stop current task
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Retry   ‚îÇ ‚Üê Retry from current state
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇüé§ Voice ‚îÇ ‚Üê Toggle voice input
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Quit    ‚îÇ ‚Üê Exit application
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Voice Indicator** (Top Center)
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üé§ Listening... ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```


##üèóÔ∏è Architecture

###System Components

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Vision AI Assistant                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   User       ‚îÇ    ‚îÇ   Voice      ‚îÇ    ‚îÇ   Screen     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   Input      ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ Recognition  ‚îÇ    ‚îÇ   Capture    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  (PyQt5)     ‚îÇ    ‚îÇ  (Speech_R)  ‚îÇ    ‚îÇ    (MSS)     ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ         ‚îÇ                                         ‚îÇ         ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                           ‚ñº                                 ‚îÇ
‚îÇ                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ
‚îÇ                  ‚îÇ   Task Context   ‚îÇ                      ‚îÇ
‚îÇ                  ‚îÇ    Management    ‚îÇ                      ‚îÇ
‚îÇ                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ
‚îÇ                           ‚îÇ                                 ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ         ‚ñº                 ‚ñº                 ‚ñº              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ OmniParser  ‚îÇ  ‚îÇ   Gemini    ‚îÇ  ‚îÇ  PyAutoGUI  ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   (API)     ‚îÇ  ‚îÇ  2.5 Flash  ‚îÇ  ‚îÇ  (Control)  ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ             ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ UI Element  ‚îÇ  ‚îÇ Decision    ‚îÇ  ‚îÇ Click/Type  ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ Detection   ‚îÇ  ‚îÇ Making      ‚îÇ  ‚îÇ Scroll      ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

###Data Flow

```
1. Input Stage
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Voice Input ‚îÇ ‚îÄ‚îÄ‚îê
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
                      ‚îú‚îÄ‚îÄ‚Üí [Task Definition]
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
   ‚îÇ Text Input  ‚îÇ ‚îÄ‚îÄ‚îò
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

2. Analysis Stage
   [Task Definition]
         ‚Üì
   [Capture Screen] ‚Üí [Screenshot.png]
         ‚Üì
   [OmniParser API] ‚Üí [Annotated Image + Element List]
         ‚Üì
   [Gemini 2.5 Flash] ‚Üí [Action Plan (JSON)]

3. Execution Stage
   [Action Plan]
         ‚Üì
   [For each step]:
      ‚Ä¢ Parse step type
      ‚Ä¢ Execute action (click/type/scroll)
      ‚Ä¢ Monitor screen change
      ‚Ä¢ Wait for stabilization
      ‚Ä¢ Capture new state
         ‚Üì
   [Next step or re-analyze]

4. Completion
   [All steps done]
         ‚Üì
   [Show completion message]
         ‚Üì
   [Ready for next task]
```

###JSON Response Format

```json
{
  "steps": [
    {
      "type": "click",
      "element_number": 5,
      "description": "Click Chrome browser",
      "double_click": false
    },
    {
      "type": "wait_and_send_image",
      "description": "Wait for Chrome to open"
    },
    {
      "type": "keyboard",
      "content": "youtube.com",
      "element_number": 3,
      "description": "Type YouTube URL"
    },
    {
      "type": "end",
      "message": "Successfully opened YouTube"
    }
  ]
}
```

```
##üöÄ Installation

###Prerequisites

- **Python**: 3.8 or higher
- **Operating System**: Windows (primary), Linux/Mac (experimental)
- **Internet**: Required for Google Speech Recognition and Gemini API
- **Microphone**: Required for voice input

###Step 1: Clone Repository

```bash
git clone https://github.com/yourusername/vision-ai-assistant.git
cd vision-ai-assistant
```

###Step 2: Create Virtual Environment (Recommended)

```bash
#Windows
python -m venv venv
venv\Scripts\activate

#Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

###Step 3: Install Dependencies

```bash
pip install --upgrade pip
pip install PyQt5 mss pillow google-generativeai pyautogui pynput requests python-dotenv numpy
```

###Step 4: Install Speech Recognition

```bash
#Main package
pip install SpeechRecognition

#For Windows - PyAudio
pip install pipwin
pipwin install pyaudio

#OR download wheel from: https://www.lfd.uci.edu/~gohlke/pythonlibs/#pyaudio
#Then: pip install PyAudio‚Äë0.2.11‚Äëcp311‚Äëcp311‚Äëwin_amd64.whl

#For offline mode (optional)
pip install pocketsphinx
```

###Step 5: Setup Environment Variables

Create a `.env` file in the project root:

```env
gemini_key=YOUR_GEMINI_API_KEY_HERE
```

**Get Gemini API Key:**
1. Go to: https://aistudio.google.com/app/apikey
2. Create new API key
3. Copy and paste into `.env` file

###Step 6: Setup OmniParser

OmniParser requires a separate service. Options:

**Option A: Use Cloudflare Tunnel (Provided in code)**
```python
OMNIPARSER_URL = "https://settings-referred-belongs-scenic.trycloudflare.com/process"
```

**Option B: Run Your Own OmniParser Server**
```bash
#Clone OmniParser repository
git clone https://github.com/microsoft/OmniParser.git
cd OmniParser

#Follow their setup instructions
#Update OMNIPARSER_URL in code to your local endpoint
```

###Step 7: Verify Installation

```bash
python vision_assistant.py
```

You should see:
```
üé§ Adjusting for ambient noise...
‚úì Voice recognition ready
```

```
##‚öôÔ∏è Configuration

###Main Configuration Variables

Edit these at the top of `vision_assistant.py`:

```python
#===== API Configuration =====
API_KEY = os.getenv("gemini_key")           #Gemini API key from .env
MODEL_NAME = "gemini-2.5-flash"              #AI model to use
OMNIPARSER_URL = "https://your-url.com"      #OmniParser endpoint

#===== Timing Configuration =====
HIDE_AND_CAPTURE_DELAY_MS = 120              #Delay before screenshot (ms)
SCREEN_CHANGE_THRESHOLD = 0.05               #5% change detection
BUFFER_DELAY_MS = 500                        #Wait after change detected
MAX_WAIT_FOR_CHANGE = 4000                   #Max wait for screen change

#===== Voice Configuration =====
USE_OFFLINE_RECOGNITION = False              #False=Google, True=Sphinx
WAKE_WORD = "hey vision"                     #Wake word phrase
SILENCE_DURATION = 1.5                       #Seconds before stop recording
VOICE_ENABLED = True                         #Start with voice on/off
AUTO_SUBMIT_VOICE = False                    #Auto-submit after recognition
AUTO_SUBMIT_DELAY_MS = 1000                  #Delay before auto-submit
```

###Voice Recognition Modes

| Mode | Accuracy | Speed | Internet | Setup Difficulty |
|------|----------|-------|----------|------------------|
| **Google** (Recommended) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Fast | Required | Easy |
| **Sphinx** (Offline) | ‚≠ê‚≠ê‚≠ê | Medium | Not needed | Medium |

###Adjusting Sensitivity

**Screen Change Detection:**
```python
SCREEN_CHANGE_THRESHOLD = 0.05  #Lower = more sensitive (0.01 - 0.1)
```

**Voice Recognition:**
```python
#In VoiceRecognitionThread.__init__()
self.recognizer.energy_threshold = 4000      #Lower = more sensitive
self.recognizer.pause_threshold = 1.5        #Shorter = faster detection
```


##üìñ Usage Guide

###Basic Usage

**Method 1: Voice Command (Wake Word)**
```
1. Launch application
2. Wait for "Ready" message
3. Say: "Hey Vision"
4. Wait for listening indicator
5. Say your command: "Open Chrome and search for Python tutorials"
6. System executes automatically
```

**Method 2: Voice Command (Manual Button)**
```
1. Launch application
2. Click üéôÔ∏è Speak button
3. Say your command
4. Press Enter (or wait 1s for auto-submit if enabled)
```

**Method 3: Text Input**
```
1. Launch application
2. Type command in input field
3. Press Enter
```

###Example Commands

####Simple Tasks
```
‚úÖ "Open Chrome"
‚úÖ "Search for weather"
‚úÖ "Open calculator"
‚úÖ "Close this window"
```

####Complex Tasks
```
‚úÖ "Open Chrome and search for best Italian restaurants near me"
‚úÖ "Go to YouTube and play Lata Mangeshkar songs"
‚úÖ "Open Gmail and compose new email"
‚úÖ "Search for Python tutorials on Google"
```

####Multi-Step Tasks
```
‚úÖ "Open Chrome, go to GitHub, search for AI projects, and open the first result"
‚úÖ "Open calculator and calculate 25 * 37"
‚úÖ "Go to Amazon, search for wireless headphones under $50"
```

###Control Panel Usage

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Abort   ‚îÇ Stop current task     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Retry   ‚îÇ Retry from last state ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇüé§ Voice ‚îÇ Toggle voice on/off   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Quit    ‚îÇ Exit application      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

###Status Indicators

| Status | Meaning |
|--------|---------|
| üîÑ **Capturing...** | Taking screenshot |
| üîç **Analyzing Screen...** | Processing with OmniParser |
| üß† **Thinking...** | AI planning next steps |
| ‚ö° **Executing...** | Performing actions |
| üé§ **Listening...** | Recording voice input |
| ‚è≥ **Waiting...** | Monitoring screen changes |
| ‚úÖ **Complete!** | Task finished |

```
##üîå API Integration

###OmniParser API

**Endpoint:**
```
POST https://your-omniparser-url/process
```

**Request:**
```python
files = {"image": open("screenshot.png", "rb")}
data = {
    "box_threshold": 0.05,    #Detection confidence
    "iou_threshold": 0.1      #Overlap threshold
}
```

**Response:**
```json
{
  "parsed_content": [
    {
      "bbox": [0.1, 0.2, 0.3, 0.4],
      "content": "Chrome Browser",
      "type": "icon"
    }
  ],
  "image_base64": "iVBORw0KGgoAAAANS..."
}
```

###Gemini API

**Configuration:**
```python
genai.configure(api_key=API_KEY)
model = genai.GenerativeModel("gemini-2.5-flash")
```

**Request:**
```python
response = model.generate_content([
    "Analyze this screen and provide next steps...",
    PIL_Image_Object
])
```

**Response Format:**
```json
{
  "steps": [
    {
      "type": "click|keyboard|scroll|wait_and_send_image|ask_question|end",
      ...additional fields...
    }
  ]
}
```

###Step Types Reference

| Type | Description | Required Fields |
|------|-------------|-----------------|
| **click** | Click UI element | `element_number`, `description`, `double_click` |
| **keyboard** | Type text | `content`, `element_number`, `description` |
| **scroll** | Scroll page | `magnitude`, `description` |
| **wait_and_send_image** | Wait and re-analyze | `description` |
| **ask_question** | Ask user | `question`, `description` |
| **end** | Complete task | `message`, `description` |

```
##üêõ Troubleshooting

###Common Issues

####1. Microphone Not Working

**Symptoms:** No wake word detection, voice recognition fails

**Solutions:**
```bash
#Test microphone
python -c "import speech_recognition as sr; print(sr.Microphone.list_microphone_names())"

#Adjust sensitivity in code
self.recognizer.energy_threshold = 300  #Lower value
```

####2. PyAudio Installation Fails

**Windows:**
```bash
#Method 1: pipwin
pip install pipwin
pipwin install pyaudio

#Method 2: Download wheel
#Get from: https://www.lfd.uci.edu/~gohlke/pythonlibs/#pyaudio
pip install PyAudio‚Äë0.2.11‚Äëcp311‚Äëcp311‚Äëwin_amd64.whl
```

**Linux:**
```bash
sudo apt-get install portaudio19-dev python3-pyaudio
pip install pyaudio
```

**Mac:**
```bash
brew install portaudio
pip install pyaudio
```

####3. Screen Capture Issues

**Symptoms:** Screenshots are black or empty

**Solutions:**
```python
#Try different monitor
monitor = sct.monitors[0]  #Change index

#Check permissions (Mac)
#System Preferences ‚Üí Security & Privacy ‚Üí Screen Recording
```

####4. API Errors

**OmniParser Connection Failed:**
```
- Check OMNIPARSER_URL is correct
- Verify service is running
- Check firewall/network settings
```

**Gemini API Error:**
```
- Verify API key in .env file
- Check API quota: https://aistudio.google.com/app/apikey
- Ensure internet connection
```

####5. Element Click Misses Target

**Symptoms:** Clicks wrong location

**Solutions:**
```python
#Adjust thresholds in OmniParser call
call_omniparser(
    image_path,
    box_threshold=0.03,  #Lower = more elements
    iou_threshold=0.15   #Higher = less overlap
)
```

###Debug Mode

Enable detailed logging:

```python
#Add at top of file
import logging
logging.basicConfig(level=logging.DEBUG)

#Or add print statements
print(f"DEBUG: Element {elem_num} at position {click_x}, {click_y}")
```

###Log Files

The application generates these files for debugging:

```
annotated_screen_*.png       #Annotated screenshots with element numbers
gemini_prompt_*.txt          #Full prompts sent to Gemini
gemini_response_*.txt        #AI responses
current_screen.png           #Latest screenshot
last_stable_screen.png       #Last verified stable screen
```

```
##üî¨ Advanced Usage

###Custom System Prompt

Edit `SYSTEM_PROMPT` variable to customize AI behavior:

```python
SYSTEM_PROMPT = """You are a virtual assistant specialized in...
- Custom rule 1
- Custom rule 2
"""
```

###Adding New Step Types

1. **Define step type in prompt:**
```python
7. "custom_action" - Your custom action
   {
     "type": "custom_action",
     "param1": "value",
     "description": "What it does"
   }
```

2. **Add handler in `_execute_next_step()`:**
```python
elif step_type == "custom_action":
    self._execute_custom_action(step)
```

3. **Implement handler method:**
```python
def _execute_custom_action(self, step):
    #Your implementation
    pass
```

###Multi-Monitor Support

```python
#In capture_and_process()
with mss.mss() as sct:
    monitor = sct.monitors[2]  #Change to desired monitor (1, 2, 3...)
    img = sct.grab(monitor)
```

###Custom Voice Commands

```python
#In _listen_for_wake_word()
if "custom wake word" in text:
    #Trigger custom action
    pass
```

```
##ü§ù Contributing

We welcome contributions! Here's how you can help:

###Ways to Contribute

1. **üêõ Report Bugs**
   - Use GitHub Issues
   - Include error logs
   - Provide steps to reproduce

2. **üí° Suggest Features**
   - Open a feature request
   - Explain use case
   - Provide examples

3. **üìù Improve Documentation**
   - Fix typos
   - Add examples
   - Translate to other languages

4. **üîß Submit Code**
   - Fork repository
   - Create feature branch
   - Submit pull request

###Development Setup

```bash
#Fork and clone
git clone https://github.com/yourusername/vision-ai-assistant.git
cd vision-ai-assistant

#Create branch
git checkout -b feature/your-feature-name

#Make changes and test
python vision_assistant.py

#Commit and push
git add .
git commit -m "Add: your feature description"
git push origin feature/your-feature-name

#Create Pull Request on GitHub
```

###Code Style

- Follow PEP 8
- Add docstrings to functions
- Comment complex logic
- Use type hints where possible

```python
def example_function(param: str, threshold: float = 0.5) -> bool:
    """
    Brief description of function.
    
    Args:
        param: Description of param
        threshold: Description of threshold
        
    Returns:
        Description of return value
    """
    pass
```

```
##üìä Performance Tips

###Optimize Speed

```python
#Reduce capture delay
HIDE_AND_CAPTURE_DELAY_MS = 50  #Faster but less reliable

#Reduce buffer delay
BUFFER_DELAY_MS = 300  #Faster transitions

#Adjust threshold for quicker detection
SCREEN_CHANGE_THRESHOLD = 0.03  #More sensitive
```

###Reduce API Calls

```python
#Batch operations in single prompt
"Click element 5, type 'hello', then click element 8"

#Use wait_and_send_image strategically
#Only when screen will definitely change
```

###Memory Management

```python
#Clean old files periodically
import glob
for f in glob.glob("annotated_screen_*.png"):
    if file_age > 1_hour:
        os.remove(f)
```

```
##üîí Security & Privacy

###Data Handling

- **Screenshots**: Stored locally, deleted on restart
- **Voice Data**: Processed in real-time, not stored
- **API Keys**: Kept in `.env`, never committed to Git

###Best Practices

1. **Never commit `.env` file**
```bash
#Add to .gitignore
.env
*.png
gemini_*.txt
```

2. **Use API key with restrictions**
   - Set usage quotas
   - Restrict to specific IPs if possible
   - Monitor usage at: https://aistudio.google.com

3. **Review screenshots before sharing**
   - May contain sensitive information
   - Check annotated images for personal data

```
##üìú License

```
MIT License

Copyright (c) 2024 [Your Name]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

```
##üôè Acknowledgments

- **OmniParser** - Microsoft Research for UI element detection
- **Google Gemini** - Multimodal AI capabilities
- **PyQt5** - Cross-platform GUI framework
- **SpeechRecognition** - Voice input handling
- **Community Contributors** - For feedback and improvements

```
##üìû Contact & Support

- **GitHub Issues**: [Report bugs or request features](https://github.com/yourusername/vision-ai-assistant/issues)
- **Discussions**: [Ask questions and share ideas](https://github.com/yourusername/vision-ai-assistant/discussions)
- **Email**: your.email@example.com
- **Twitter**: @yourusername

```
##üó∫Ô∏è Roadmap

###Version 1.0 (Current)
- ‚úÖ Screen understanding with OmniParser
- ‚úÖ Voice input with wake word
- ‚úÖ Basic task automation
- ‚úÖ Real-time screen monitoring

###Version 1.1 (Planned)
- üî≤ Multi-monitor support
- üî≤ Task templates/macros
- üî≤ Scheduled automation
- üî≤ Browser extension integration

###Version 2.0 (Future)
- üî≤ Self-learning from user corrections
- üî≤ Plugin system for custom actions
- üî≤ Mobile app companion
- üî≤ Cloud sync for tasks
- üî≤ Collaborative automation sharing

```
##üìà Project Stats

![GitHub Stars](https://img.shields.io/github/stars/yourusername/vision-ai-assistant?style=social)
![GitHub Forks](https://img.shields.io/github/forks/yourusername/vision-ai-assistant?style=social)
![GitHub Issues](https://img.shields.io/github/issues/yourusername/vision-ai-assistant)
![GitHub Pull Requests](https://img.shields.io/github/issues-pr/yourusername/vision-ai-assistant)

```
<div align="center">

**If you find this project helpful, please consider giving it a ‚≠ê!**

Made with ‚ù§Ô∏è by [Your Name]

</div>

```
